<DOC>
<DOCNO>ABST-419</DOCNO>
<TEXT>A Few Examples Go A Long Way: Constructing Query Models from Elaborate Query Formulations We address a specific enterprise document search scenario, where the information need is expressed in an elaborate manner.  In our scenario, information needs are expressed using a short query (of a few keywords) together with examples of key reference pages. Given this setup, we investigate how the examples can be utilized to improve the end-to-end performance on the document retrieval task.  Our approach is based on a language modeling framework, where the query model model is modified to resemble the example pages.  We compare several methods for sampling expansion terms from the example pages to support query-dependent and query-independent query expansion; the latter is motivated by the wish to increase ``aspect recall,'' and attempts to uncover aspects of the information need not captured by the query. For evaluation purposes we use the CSIRO data set created for the TREC 2007 Enterprise track.  The best performance is achieved by query models based on query-independent sampling of expansion terms from the example documents.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-713</DOCNO>
<TEXT>Classification of Dual Language Audio-Visual Content: Introduction to the VideoCLEF 2008 Pilot Benchmark Evaluation Task VideoCLEF is a new track for the CLEF 2008 campaign. This track aims to develop and evaluate tasks in analyzing multilingual video content. A pilot of a Vid2RSS task involving assigning thematic class labels to video kicks off the VideoCLEF track in 2008. Task participants deliver classification results in the form of a series of feeds, one for each thematic class. The data for the task are dual language television documentaries. Dutch is the dominant language and English-language content (mostly interviews) is embedded. Participants are provided with speech recognition transcripts of the data in both Dutch and English, and also with metadata generated by archivists. In addition to the classification task, participants can choose to participate in a translation task (translating the feed into a language of their choice) and a keyframe selection task (choosing a semantically appropriate keyframe for depiction of the videos in the feed).</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-487</DOCNO>
<TEXT>Language Modeling Approaches to Blog Post and Feed Finding We describe our participation in the TREC 2007 Blog track. In the opinion task we looked at the differences in performance between Indri and our mixture model, the influence of external expansion and document priors to improve opinion finding; results show that an out-of-the-box Indri implementation outperforms our mixture model, and that external expansion on a news corpus is very benificial. Opinion finding can be improved using either lexicons or the number of comments as document priors. Our approach to the feed distillation task is based on aggregating post-level scores to obtain a feed-level ranking. We integrated time-based and persistence aspects into the retrieval model. After correcting bugs in our post-score aggregation module we found that time-based retrieval improves results only marginally, while persistence-based ranking results in substantial improvements under the right circumstances.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-430</DOCNO>
<TEXT>Bloggers as Experts We address the task of (blog) feed distillation: to find blogs that are principally devoted to a given topic.  The task may be viewed as an association finding task, between topics and bloggers; it resembles the expert finding task, for which a range of models have been proposed. We adopt two language modeling-based approaches to expert finding, and determine their effectiveness as feed distillation strategies. The two models capture the idea that a human will often search for key blogs by spotting highly relevant posts (the Posting model) or by taking global aspects of the blog into account (the Blogger model). The Blogger model outperforms the Posting model and delivers state-of-the art performance, out-of-the-box.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-485</DOCNO>
<TEXT>Entity Models for Trigger-Reaction Documents We define the notion of an entity model for a special kind of document popular on the web: an article followed by a list of reactions on that article, usually by many authors, usually inverse chronologically ordered. We call these documents trigger-reactions pairs. The entity model describes which  named entities (persons, organizations, locations, products, urls) are mentioned, their type, how often and where they are mentioned, and it lists all variants referring to the same entity. These models find applications in media-analysis, trend watching, entity tracking and marketing. The two main challenges for creating entity models are 1) detecting the entities and 2) normalizing all variants to the same correct canonical form. This task is particularly hard for user generated content on the web, of which our reactions are an example. We use an algorithm for named entity recognition and normalization (NEN) tailor-made for trigger-reaction documents. It achieves high recall and reasonable precision by using two simple facts: 1) incomplete entities in reactions often occur complete in the trigger and 2) entities mentioned in news-articles on the web often have a Wikipedia page. This article describes our experience in creating and using entity models on a corpus of 56,449 Dutch trigger-reaction documents, with a total of 616,715 reactions, collected from the web from November 11, 2006 to  February 5, 2008. This paper accompanies an earlier article from our group in which the focus was on a systems-evaluation of the  NEN algorithm.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-721</DOCNO>
<TEXT>The University of Amsterdam?s Question Answering System at QA@CLEF 2007 We describe a new version of our question answering system, which was applied to the questions of the 2007 CLEF Question Answering Dutch monolingual task. This year, we made three major modifications to the system: (1) we added the contents of Wikipedia to the document collection and the answer tables; (2) we completely rewrote the module interface code in Java; and (3) we included a new table stream which returned answer candidates based on information which was learned from question-answer pairs. Unfortunately, the changes did not lead to improved performance. Unsolved technical problems at the time of the deadline have led to missing justifications for a large number of answers in our submission. Our single run obtained an accuracy of only 8% with an additional 12% of unsupported answers (compared to 21% in the last year?s task).</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-341</DOCNO>
<TEXT>An Experiment in Automatic Classification of Pathological Reports Medical reports are predominantly written in natural language; as such they are not computer-accessible.  A common way to make medical narrative accessible to automated systems is by assigning `computer-understandable' keywords from a controlled vocabulary. Experts usually perform this task by hand.  In this paper, we investigate methods to support or automate this type of medical classification.  We report on experiments using the PALGA data set, a collection of 14 million pathological reports, each of which has been classified by a domain expert.  We describe methods for automatically categorizing the documents in this data set in an accurate way.  In order to evaluate the proposed automatic classification approaches, we compare their output with that of two additional human annotators. While the automatic system performs well in comparison with humans, the inconsistencies within the annotated data constrain the maximum attainable performance.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-355</DOCNO>
<TEXT>Expanding Queries Using Multiple Resources This paper describes our participation in the TREC 2005 Genomics track. We took part in the ad hoc retrieval task and aimed at integrating thesauri in the retrieval model. We developed three thesauri-based methods, two of which make use of the existing MeSH thesaurus. One method uses blind relevance feedback on MeSH terms, the second uses an index of the MeSH thesaurus for query expansion. The third method makes use of a dynamically generated lookup list, by which acronyms and synonyms could be inferred. We show that, despite the relatively minor improvements in retrieval performance of individually applied methods, a combination works best and is able to deliver significant improvements over the baseline.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-377</DOCNO>
<TEXT>Extracting the Discussion Structure in Comments on News-Articles Several on-line daily newspapers offer readers the opportunity to directly comment on articles. In the Netherlands this feature is used quite often and the quality (grammatically and content-wise) is surprisingly high. We develop techniques to collect, store, enrich and analyze these comments. After giving a high-level overview of the Dutch commento-sphere we zoom in on extracting the discussion structure found in flat comment threads; people not only comment on the news article, they also heavily comment on other comments, resembling discussion fora. We show how techniques from information retrieval, natural language processing and machine learning can be used to extract the reacts-on relation between comments with high precision and recall.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-436</DOCNO>
<TEXT>Finding Key Bloggers, One Post At A Time User generated content in general, and blogs in particular, form an interesting and relatively little explored domain for mining knowledge.  We address the task of blog distillation: to find blogs that are principally devoted to a given topic, as opposed to blogs that merely happen to discuss the topic in passing.  Working in the setting of statistical language modeling, we model the task by aggregating a blogger's blog posts to collect evidence of relevance to the topic and persistence of interest in the topic. This approach achieves state-of-the-art performance.  On top of this baseline, we extend our model by incorporating a number of blog-specific features, concerning document structure, social structure, and temporal structure.  These blog-specific features yield further improvements.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-420</DOCNO>
<TEXT>Credibility Improves Topical Blog Post Retrieval Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic. To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process. We consider two groups of indicators: post level (determined using information about individual blog posts only) and blog level (determined using information from the underlying blogs). We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models. Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness; the best performance is achieved when combining them.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-422</DOCNO>
<TEXT>Looking at Things Differently: Exploring Perspective Recall for Informal Text Retrieval When retrieving informal text such as blogs, comments, contributions to discussion forums, users often want to uncover different perspectives on a given issue. To help uncover perspectives, we examine the use of query expansion against multiple external corpora. We consider two informal text retrieval tasks: blog post finding and blog finding. We operationalize the idea of uncovering multiple perspectives by query expansion against multiple corpora from different genres. We use two approaches to incorporate these perspectives: as a rank-based combination of runs and a mixture of query models. The use of external sources does indeed generate different views on a topic as becomes clear from the unique relevant results identified by the expanded runs compared to the baseline run. Even after combining the expanded run with the original run, unique relevant documents are found by both of the perspectives. As to the combination methods, the mixture of query models outperforms the rank combination, and leads to significant improvements in MAP score over the baseline</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-423</DOCNO>
<TEXT>The Impact of Named Entity Normalization on Information Retrieval for Question Answering In the named entity normalization task, a system identifies a canonical unambiguous referent for names like "Bush" or "Alabama."  Resolving synonymy and ambiguity of such names can benefit end-to-end information access tasks.  We evaluate two entity normalization methods based on Wikipedia in the context of both passage and document retrieval for question answering.  We find that even a simple normalization method leads to improvements of early precision, both for document and passage retrieval.  Moreover, better normalization results in better retrieval performance.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-425</DOCNO>
<TEXT>Associating People and Documents Since the introduction of the Enterprise Track at TREC in 2005, the task of finding experts has generated a lot of interest within the research community.  Numerous models have been proposed that rank candidates by their level of expertise with respect to some topic.  Common to all approaches is a component that estimates the strength of the association between a document and a person.  Forming such associations, then, is a key ingredient in expertise search models.  In this paper we introduce and compare a number of methods for building document-people associations.  Moreover, we make underlying assumptions explicit, and examine two in detail: (i) independence of candidates, and (ii) frequency is an indication of strength. We show that our refined ways of estimating the strength of associations between people and documents leads to significant improvements over the state-of-the-art in the end-to-end expert finding task.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-431</DOCNO>
<TEXT>Parsimonious Concept Modeling We  introduce a parsimonious conceptual query model whose  retrieval performance matches that of relevance models, while it  is also able to generate high quality navigation suggestions in the  form of concepts. Future work concerns further experimental validation of our results on additional test collections, as well as revisiting the modeling assumptions we made.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-433</DOCNO>
<TEXT>Parsimonious Relevance Models We describe a method for applying parsimonious language models to re-estimate the term probabilities assigned by relevance models. We apply our method to six topic sets from test collections in five different genres.  Our parsimonious relevance models (i) improve retrieval effectiveness in terms of MAP on all collections, (ii) significantly outperform their non-parsimonious counterparts on most measures, and (iii) have a precision enhancing effect, unlike other blind relevance feedback methods.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-432</DOCNO>
<TEXT>Term Clouds as Surrogates for User Generated Speech User generated spoken audio remains a challenge for Automatic Speech Recognition (ASR) technology and content-based audio surrogates derived from ASR-transcripts must be error robust. An investigation of the use of term clouds as surrogates for podcasts demonstrates that ASR term clouds closely approximate term clouds derived from human-generated transcripts across a range of cloud sizes.  A user study confirms the conclusion that ASR-clouds are viable surrogates for depicting the content of podcasts.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-434</DOCNO>
<TEXT>Personal vs Non-Personal Blogs: Initial Classification Experiments We address the task of separating personal from non-personal blogs, and report on a set of baseline experiments where we compare the performance on  a small set of features across a set of five classifiers. We show that with a limited set of features a performance of up to 90% can be obtained.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-435</DOCNO>
<TEXT>Measuring Concept Relatedness Using Language Models Over the years, the notion of concept relatedness has attracted considerable attention.  A variety of approaches, based on ontology structure, information content, association, or context have been proposed to indicate the relatedness of abstract ideas. We propose a method based on the cross entropy reduction between language models of concepts which are estimated based on document-concept assignments.  The approach shows improved or competitive results compared to state-of-the-art methods on two test sets in the biomedical domain.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-437</DOCNO>
<TEXT>Named Entity Normalization in User Generated Content Named entity recognition is important for semantically oriented retrieval tasks, such as question answering, entity retrieval, biomedical retrieval, trend detection, and event and entity tracking.  In many of these tasks it is important to be able to accurately normalize the recognized entities, i.e., to map surface forms to unambiguous references to real world entities.  Within the context of structured databases, this task (known as record linkage and data de-duplication) has been a topic of active research for more than five decades.  For edited content, such as news articles, the named entity normalization (NEN) task is one that has recently attracted considerable attention.  We consider the task in the challenging context of user generated content (UGC), where it forms a key ingredient of tracking and media-analysis systems. A baseline NEN system from the literature (that normalizes surface forms to Wikipedia pages) performs considerably worse on UGC than on edited news: accuracy drops from 80% to 65% for a Dutch language data set and from 94% to 77% for English.  We identify several sources of errors: entity recognition errors, multiple ways of referring to the same entity and ambiguous references. To address these issues we propose five improvements to the baseline NEN algorithm, to arrive at a language independent NEN system that achieves overall accuracy scores of 90% on the English data set and 89% on the Dutch data set.  We show that each of the improvements contributes to the overall score of our improved NEN algorithm, and conclude with an error analysis on both Dutch and English language UGC. The NEN system is computationally efficient and runs with very modest computational requirements.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-438</DOCNO>
<TEXT>Blogger, Stick to your Story: Modeling Topical Noise in Blogs with Coherence Measures Topical noise in blogs arises when bloggers digress from the central topical thrust of their blogs.  We introduce a method to explicitly incorporate a model of topical noise into a language modeling approach to the task of blog distillation.  Topical noise is integrated into the model using a coherence score, which reflects the tightness of the topical structure of a blog.  Tests performed on the TRECBlog06 corpus show that a naive integration of the coherence score as blog prior fails to achieve performance improvements.  Instead, we develop a set of more sophisticated models in which the coherence score is weighted by a function of the blog retrieval score.  The proposed models help improve effectiveness of our language modeling approach to the blog distillation task.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-439</DOCNO>
<TEXT>Integrating Contextual Factors into Topic-Centric Retrieval Models for Finding Similar Experts Expert finding has been addressed from multiple viewpoints, including expertise seeking and expert retrieval.  The focus of expertise seeking has mostly been on descriptive or predictive models, for example to identify what factors affect human decisions on locating and selecting experts.  In expert retrieval the focus has been on algorithms similar to document search, which identify topical matches based on the content of documents associated with experts. We report on a pilot study on an expert finding task in which we explore how contextual factors identified by expertise seeking models can be integrated with topic-centric retrieval algorithms and examine whether they can improve retrieval performance for this task.  We focus on the task of similar expert finding: given a small number of example experts, find similar experts.  Our main finding is that, while topical knowledge is the most important factor, human subjects also consider other factors, such as reliability, up-to-dateness, and organizational structure.  We find that integrating these factors into topical retrieval models can significantly improve retrieval performance.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-440</DOCNO>
<TEXT>Using Term Clouds to Represent Segment-Level Semantic Content of Podcasts Spoken audio, like any time-continuous medium, is notoriously difficult to browse or skim without support of an interface providing semantically annotated jump points to signal the user where to listen in.  Creation of time-aligned metadata by human annotators is prohibitively expensive, motivating the investigation of representations of segment-level semantic content based on transcripts generated by automatic speech recognition (ASR).  This paper examines the feasibility of using term clouds to provide users with a structured representation of the semantic content of podcast episodes.  Podcast episodes are visualized as a series of sub-episode segments, each represented by a term cloud derived from a transcript generated by automatic speech recognition (ASR).  Quality of segment-level term clouds is measured quantitatively and their utility is investigated using a small-scale user study based on human labeled segment boundaries.  Since the segment-level clouds generated from ASR-transcripts prove useful, we examine an adaptation of text tiling techniques to speech in order to be able to generate segments as part of a completely automated indexing and structuring system for browsing of spoken audio. Results demonstrate that the segments generated are comparable with human selected segment boundaries.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-443</DOCNO>
<TEXT>Assessing Concept Selection for Video Retrieval We explore the use of benchmarks to address the problem of assessing concept selection in video retrieval systems. Two benchmarks are presented, one created by human association of queries to concepts, the other generated from an extensively tagged collection. They are compared in terms of reliability, captured semantics, and retrieval performance. Recommendations are given for using the benchmarks to assess concept selection algorithms; the assessment is demonstrated on two existing algorithms. The benchmarks are released to the research community.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-488</DOCNO>
<TEXT>Non-Local Evidence for Expert Finding The task addressed in this paper, finding experts in an enterprise setting, has gained in importance and interest over the past few years.  Commonly, this task is approached as an association finding exercise between people and topics.  Existing techniques use either documents (as a whole) or proximity-based techniques to represent candidate experts. Proximity-based techniques have shown clear precision-enhancing benefits.  We complement both document and proximity-based approaches to expert finding by importing global evidence of expertise, i.e., evidence obtained using information that is not available in the immediate proximity of a candidate expert's name occurrence or even on the same page on which the name occurs.  Examples include candidate priors, query models, as well as other documents a candidate expert is associated with. Using the CERC data set created for the TREC 2007 Enterprise track we identify examples of non-local evidence of expertise.  We then propose modified expert retrieval models that are capable of incorporating both local (either document or snippet-based) evidence and non-local evidence of expertise.  Results show that our refined models significantly outperform existing state-of-the-art approaches.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-486</DOCNO>
<TEXT>Passage Retrieval for Question Answering using Sliding Windows The information retrieval (IR) community has investigated  many different techniques to retrieve passages from large collections of documents for question answering (QA). In this paper, we specifically examine and  quantitatively compare the impact of passage retrieval for QA using sliding windows and disjoint windows. We consider two different data sets, the TREC 2002--2003 QA data set, and 93 why-questions against INEX Wikipedia. We discovered that, compared to disjoint windows, using sliding windows results in improved performance of TREC-QA in terms of TDRR, and in improved performance of why-QA in terms of success@n and MRR.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-489</DOCNO>
<TEXT>Using Centrality to Rank Web Snippets We describe our participation in the WebCLEF 2007 task, targeted at snippet retrieval from web data. Our system ranks snippets based on a simple similarity-based centrality, inspired by the web page ranking algorithms. We experimented with retrieval units (sentences and paragraphs) and with the similarity functions used for centrality computations (word overlap and cosine similarity). We found that using paragraphs with the cosine similarity function shows the best performance with precision around 20% and recall around 25% according to human assessments of the ?rst 7,000 bytes of responses for individual topics.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-490</DOCNO>
<TEXT>Overview of WebCLEF 2007 This paper describes the WebCLEF 2007 task. The task definition which goes beyond traditional navigational queries and is concerned with undirected information search goals---combines insights gained at previous editions of WebCLEF and of the WiQA pilot that was run at CLEF 2006. We detail the task, the assessment procedure and the results achieved by the participants.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-491</DOCNO>
<TEXT>PodCred: A Framework for Analyzing Podcast Preference The PodCred framework is a framework for assessing the credibility and quality of podcasts published on the internet. It consists of a series of indicators designed to support prediction of listener preference of one podcast over another, given that both carry comparable informational content. The indicators are grouped into four categories pertaining to the Podcast Content, the Podcaster, the Podcast Context or the Technical Execution of the podcast. We adopt the term ``cred'' as a designation encompassing both credibility (comprising trustworthiness and expertise) and qualitative acceptability to listeners. Our podcast analysis framework is inspired by work on credibility in blogs, another medium dominated by user generated content. The PodCred framework is derived from a review of the literature on credibility for other media, a survey of prescriptive standards for podcasting, and a detailed data analysis of award winning podcasts. The paper concludes with a discussion of future work in which the framework will be applied.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-492</DOCNO>
<TEXT>On the Topical Structure of the Relevance Feedback Set We investigate the topical structure of the set of documents used to expand a query in pseudo-relevance feedback (PRF).  We propose a coherence score to measure the relative topical diversity/compactness of this document set, which we call the relevance feedback set.  The coherence score captures both the topical relatedness of the members of a document set as well as the set's overall clustering structure.  We demonstrate the ability of the coherence score to capture topical structure of a document set using tests performed on synthetic data.  Then, experiments are performed which show that when queries are divided into two sets, one with high and one with low relevance feedback set coherence, two different levels of retrieval performance are observed.  In particular, our results suggests that if a query has a relevance feedback set with a high coherence score, it is apt to benefit from PRF.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-495</DOCNO>
<TEXT>An Effective Statistical Approach to Blog Post Opinion Retrieval Finding opinionated blog posts is still an open problem in information retrieval, as exemplified by the recent TREC blog tracks. Most of the current solutions involve the use of external resources and manual efforts in identifying subjective features. In this paper, we propose a novel and effective dictionary-based statistical approach, which automatically derives evidence for sub jectivity from the blog collection itself, without requiring any manual effort. Our experiments show that the proposed approach is capable of achieving remarkable and statistically significant improvements over robust baselines, including the best TREC baseline run. In addition, with relatively little computational costs, our proposed approach provides an effective performance in retrieving opinionated blog posts, which is as good as a computationally expensive approach using Natural Language Processing techniques.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-496</DOCNO>
<TEXT>The University of Amsterdam at the CLEF 2008 Domain Specific Track We describe our participation in the CLEF 2008 Domain Specific track.  The research questions we address are threefold: (i) what are the effects of estimating and applying relevance models to the domain specific collection used at CLEF 2008, (ii) what are the results of parsimonizing these relevance models, and (iii) what are the results of applying concept models for blind relevance feedback?  Parsimonization is a technique by which the term probabilities in a language model may be re-estimated based on a comparison with a reference model, making the resulting model more sparse and to the point.  Concept models are term distributions over vocabulary terms, based on the language associated with concepts in a thesaurus or ontology and are estimated using the documents which are annotated with concepts. Concept models may be used for blind relevance feedback, by first translating a query to concepts and then back to query terms.  We find that applying relevance models helps significantly for the current test collection, in terms of both mean average precision and early precision.  Moreover, parsimonizing the relevance models helps mean average precision on title-only queries and early precision on title+narrative queries.  Our concept models are able to significantly outperform a baseline query-likelihood run, both in terms of mean average precision and early precision on both title-only and title+narrative queries.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-497</DOCNO>
<TEXT>An Exploratory Study of User Goals and Strategies in Podcast Search We report on an exploratory, qualitative user study designed to identify users? goals underlying podcast search, the strategies used to gain access to podcasts, and how currently available tools influence podcast search. We employed a multi-method approach. First, we conducted an online survey to obtain broad information on overall trends and perceptions regarding podcasts in general and podcast search in particular. Second, we used a diary study and contextual interviews to gain more detailed insights into the goals and behavior of key users. We find that goals underlying podcast search may be similar to those for blog search. Study participants report searching for podcasts to look for personal opinions and ideas, and favor topics like technology, news, and entertainment. A variety of search strategies is used to gain access to interesting podcasts, such as query-based search, directed and undirected browsing, and requested and unrequested recommendations. We find indications that goals and search strategies for podcast search are strongly influenced by perceptions of available tools, most notably the perceived lack of tools for online audio search.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-709</DOCNO>
<TEXT>The University of Amsterdam at VideoCLEF 2008 The University of Amsterdam (UAms) team carried out the Vid2RSS classification task, the primary sub-task of the VideoCLEF track at CLEF 2008. This task involves the assignment of thematic category labels to dual language (Dutch/English) television episode videos. UAms chose to focus on exploiting archival metadata and speech transcripts generated by both the Dutch and English speech recognizers. Exploratory experimentation completed prior to the start of the task on external data motivated choosing a Support Vector Machine (SVM) with a linear kernel as the classifier. As a SVM toolbox to carry out the experiments, the Least Square-SVM (LS-SVM) toolbox was selected. Wikipedia was chosen as the source of the training data because it is multilingual and contains content with broad thematic coverage. The results of the experimentation showed that archival metadata improves performance of classification, but the addition of speech recognition transcripts in one or both languages does not yield performance gains. Although the overall performance of the classifiers was less than satisfactory, adequate performance was achieved in several classes, suggesting that there is concrete potential for future work to achieve performance improvements, especially if more suitable training data could be obtained.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-717</DOCNO>
<TEXT>Overview of VideoCLEF 2008: Automatic Generation of Topic-based Feeds for Dual Language Audio-Visual Content (draft) The VideoCLEF track, introduced in 2008, aims to develop and evaluate tasks related to analysis of and access to multilingual multimedia content. In its first year, VideoCLEF piloted the Vid2RSS task, whose main subtask was the classification of dual language video (Dutch-language television content featuring English-speaking experts and studio guests). The task offered two additional discretionary subtasks: feed translation and automatic keyframe extraction. Task participants were supplied with Dutch archival metadata, Dutch speech transcripts, English speech transcripts and 10 thematic category labels, which they were required to assign to the test set videos. The videos were grouped by class label into topic-based RSS-feeds, displaying title, description and keyframe for each video. Five groups participated in the 2008 VideoCLEF track. Participants were required to collect their own training data; both Wikipedia and general web content were used. Groups deployed various classifiers (SVM, Naive Bayes and k-NN) or treated the problem as an information retrieval task. Both the Dutch speech transcripts and the archival metadata performed well as sources of indexing features, but no group succeeded in exploiting combinations of feature sources to significantly enhance performance.  A small scale fluency/adequacy evaluation of the translation task output revealed the translation to be of sufficient quality to make it valuable to a non-Dutch speaking English speaker. For keyframe extraction, the strategy chosen was to select the keyframe from the shot with the most representative speech transcript content. The automatically selected shots were shown, with a small user study, to be competitive with manually selected shots. Future years of VideoCLEF will aim to expand the corpus and the class label list, as well as to extend the track to additional tasks.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-714</DOCNO>
<TEXT>Multimodal Indexing of Electronic Audio-Visual Documnets: A Case Study for Cultural Heritage Data This paper describes a multimedia multimodal information access sub-system (MIAS) for digital audio-visual documents, typically presented in streaming media format. The system is designed to provide both professional and general users with entry points into video documents that are relevant to their information needs. In this work, we focus on the information needs of multimedia specialists at a Dutch cultural heritage institution with a large multimedia archive. A quantitative and qualitative assessment is made of the efficiency of search operations using our multimodal system and it is demonstrated that MIAS significantly accelerates the information retrieval process when searching within a video document.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-719</DOCNO>
<TEXT>Access to legal documents: Exact match, best match, and combinations In this paper, we document our efforts in participating to the TREC 2007 Legal track. We had multiple aims: First, to experiment with using different query formulations, trying to exploit the verbose topic statements. Second, to analyse how ranked retrieval methods can be fruitfully combined with traditional Boolean queries. Our main ?ndings can be summarized as follows: First, we got mixed results trying to combine the original search request with terms extracted from the verbose topic statement. Second, by combining the Boolean reference run wit our ranked retrieval run allows us to get the high recall of the Boolean retrieval, whilst precision scores show an improvement over both the Boolean and the ranked retrieval runs. Third, we found out that if we treat the Boolean query as free text with varying degrees of interpretation of the original operator, we get competitive results. Moreover, both types of queries seem to capture different relevant documents, and the combination between the request text and the Boolean query leads to substantial gain in precision and recall.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-720</DOCNO>
<TEXT>Overview of WebCLEF 2008 (draft) We describe the WebCLEF 2008 task.  Similarly to the 2007 edition of WebCLEF, the 2008 edition implements a multilingual ``information synthesis" task, where, for a given topic, participating systems have to extract important snippets from web pages.  We detail the task and the assessment procedure. At the time of writing evaluation results are not available yet.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-724</DOCNO>
<TEXT>Content Extraction for Information Retrieval in Blogs and Intranets As the web keeps growing, identifying and retrieving useful information from this huge amount of data continues to be a problem. One dif?culty in web retrieval is that web pages not only contain useful content, but also navigation bars, advertisements, disclaimers, and other boilerplate material. Removing this boilerplate material, i.e. extracting useful content from web documents, has been shown to improve performance of linguistic applications such as classification and clustering. However, it is not clear how content extraction influences retrieval performance.  In this paper we systematically evaluate the impact of a simple content extraction method on retrieval performance on two web collections: (i) a blog collection and (ii) an intranet collection. Our results indicate that blog retrieval substantially benefits from even simple content extraction methods, while applying the same content extraction method to intranet documents does not improve retrieval performance over the baseline. We analyze the results per-topic to identify collection characteristics that could cause these differences. Main differences include the large amount of noise and advertisements found in the blog collection.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-733</DOCNO>
<TEXT>The University of Amsterdam at TREC 2008: Blog, Enterprise, and Relevance Feedback We describe the participation of the University of Amsterdam's ILPS group in the blog, enterprise and relevance feedback track at TREC 2008.  Our main preliminary conclusions are that estimating mixture weights for external expansion in blog post retrieval is non-trivial and we need more analysis to find out why it works better for blog distillation than for blog post retrieval.  For the relevance feedback track we observe two things: (i) in terms of statMAP, a larger number of judged non-relevant documents improves retrieval effectiveness and (ii) on the TREC Terabyte topics, we can effectively replace the estimates on the judged non-relevant documents with estimations on the document collection. Finally, since the enterprise track did not have any results yet, we only described our participation and do not draw any conclusions.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-736</DOCNO>
<TEXT>The MediaMill TRECVID 2008 Semantic Video Search Engine In this paper we describe our TRECVID 2008 video retrieval experiments. The MediaMill team participated in three tasks: concept detection, automatic search, and interactive search. Rather than continuing to increase the number of concept detectors available for retrieval, our TRECVID 2008 experiments focus on increasing the robustness of a small set of detectors. To that end, our concept detection experiments emphasize in particular the role of sampling, the value of color invariant features, the influence of codebook construction, and the effectiveness of kernel-based learning parameters. For retrieval, a robust but limited set of concept detectors necessitates the need to rely on as many auxiliary information channels as possible. Therefore, our automatic search experiments focus on predicting which information channel to trust given a certain topic, leading to a novel framework for predictive video retrieval. To improve the video retrieval results further, our interactive search experiments investigate the roles of visualizing preview results for a certain browse-dimension and active learning mechanisms that learn to solve complex search topics by analysis from user browsing behavior. The 2008 edition of the TRECVID benchmark has been the most successful MediaMill participation to date, resulting in the top ranking for both concept detection and interactive search, and a runner-up ranking for automatic retrieval. Again a lot has been learned during this year's TRECVID campaign; we highlight the most important lessons at the end of this paper.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-741</DOCNO>
<TEXT>The University of Amsterdam (ILPS) at INEX 2008 We describe our participation in the INEX 2008 Entity Ranking and Link-the-Wiki tracks. We provide a detailed account of the ideas underlying our approaches to these tasks. For the Link-the-Wiki track, we also report on the results and findings so far.</TEXT>
</DOC>

<DOC>
<DOCNO>ABST-753</DOCNO>
<TEXT>Task-based Evaluation Report: Building a Dutch Subjectivity Lexicon We describe a method for creating a Dutch subjectivity lexicon based on an English subjectivity lexicon, an online translation service and a Dutch general purpose thesaurus: Wordnet. We use a PageRank-like algorithm to bootstrap from the Dutch translation of the English lexicon and rank the words in the Dutch thesaurus by polarity. Two versions of the Dutch Wordnet are used in the experiments: the 2001 version and the 2008 version developed within the Cornetto project. We present the evaluation results based on human assessment of the top 2000 negative words and the top 1500 positive words in the resulting lexicons. We ?nd that using Cornetto results in a 7% improvement in accuracy. Between 70% to 86% of this improvement can be attributed to the larger size of Cornetto, the remaining improvement is attributed to the larger set of relations between words.</TEXT>
</DOC>
